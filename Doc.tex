\documentclass[english,11pt]{article}

\input{preambles}

\usepackage[margin=1in]{geometry}
 \usepackage{setspace}
% \setstretch{}
\usepackage{listings}

\lstset { %
    language=C++,
    backgroundcolor=\color{black!5}, % set backgroundcolor
    basicstyle=\footnotesize,% basic font setting
}

\begin{document}

\title{Computing Large Scale Linear Least Squares with Ski-LLS}

\author{Zhen Shao}\maketitle

\section{Overview}

Ski-LLS (SKetchIng-Linear-Least-Sqaures) is a C++ package for finding solutions to over-determined linear least square problems. Ski-LLS uses a modern dimensionality reduction technique called sketching, and is particularly suited for large scale linear least squares where the number of measurements/observations is far greater than the number of variables. 

Mathematically, Ski-LLS solves

\begin{align}
\min_{x \in \R^{d}} f(x): = \|Ax - b\|_2^2, \label{eq::problem}
\end{align}
where $A \in \R^{n\times d}$ and $b \in \R^{n}$ given. The matrix $A$ is allowed to be rank-deficient or nearly rank-deficient. 

\subsection{When to use Ski-LLS}

If $A$ in \eqref{eq::problem} is dense, the state-of-the-art sketching solver is Blendenpik \cite{doi:10.1137/090767911}. Comparing to solvers in the classical state-of-the-art numerical package LAPACK \cite{laug}, Blendenpik is two times faster on matrices of size $20,000 \times 500$ and four times faster on matrices of size $100,000 \times 2500$. Comparing to classical iterative solver LSQR \cite{Paige:1982aa}, Blendenpik is 80 times faster on matrices of size $20,000 \times 1,000$ with condition number $100$. 

Blendenpik only solves \eqref{eq::problem} when the matrix $A$ has full numerical rank, as illustrated in Table \ref{tab::rank_def_accuracy}. Two solvers are included in the library Ski-LLS for dense $A$. The robust version, \\{\it{ls_dense_hashing_blendenpik} }solves problems \ref{eq::problem} when $A$ is numecially rank-deficient but is as fast as Blendenpik when $A$ has full rank (takes 70\% to 130\% time on matrices of different sizes). The fast version, {\it{ls_dense_hashing_blendenpik_noCPQR}} is 1.6 times faster than Blendenpik on coherent matrices\footnote{Matrices of the form $A \in \R^{n\times d} = \bigl(\begin{smallmatrix} I_{d\times d} \\ 0  \end{smallmatrix}\bigr) + 10^{-8}J_{n,d} $ where $J_{n,d} \in \R^{n \times d}$ is a matrix of all ones.} of size $40,000 \times 4,000$ and $90,000 \times 2250$, and slightly faster than Blendenpik on other types of input (about 1.1 times faster).

If $A$ in \eqref{eq::problem} is sparse, the state-of-the-art solvers are SPQR \cite{Davis:2011ft} and HSL \cite{Scott:2014iq}, which uses sparse QR factorization of $A$ and LSQR with an incomplete Cholesky preconditioner, respectively. Our library has a single sparse solver, {\it ls_sparse_spqr}. It significantly outperforms both state-of-the-art sparse solvers on large random sparse ill-conditioned matrices. It is 10 times faster than HSL and 7 times faster than SPQR on $120,000 \times 5,000$ random sparse matrices with $1\%$ non-zero entries and condition number equals to $10^6$. And it is 3 times faster than both HSL and SPQR on $40,000 \times 2,000$ matrices with $1\%$ non-zero entries and condition number equals to $10^6$.

Our sparse solver also performs extremely well on highly over-determined sparse inputs from real applications. It is the fastest solver on more than $75 \%$ of problems in the Florida Matrix Collection \cite{Davis_2011} with the matrix $A$ defined in \eqref{eq::problem} having $n\geq 30d$ and $n \geq 20,000$. It is also the fastest solver on more than $90\%$ of problems with the matrix $A$ having $n\geq 10d$, $n \geq 20,000$ but additionally having more than $1\%$ of non-zero entries. 

Because sketching exploits redundancy of rows, the performance gain compared to non-sketching solvers\footnote{LAPACK, HSL, SPQR} will be larger on $A$ with larger $n/d$ radio.

For further details of computational advantages, see our paper. {\it{Computing Large Scale Linear Least Squares with Ski-LLS, C. Cartis and Z. Shao}}.


% Problem \eqref{eq::problem} frequently appears as a sub-problem in many computational and data science problems, for example, in many non-linear/general function minimisation routine linearises the problem and iteratively solve problems of the form \eqref{eq::problem}. In the following situations using Ski-LLS yields significant computational advantage comparing to state-of-the-arts\footnote{Blendenpik \cite{doi:10.1137/090767911} for dense problems, SPQR \cite{Davis:2011ft} and Cholesky-preconditioned LSQR \cite{Scott:2014iq} for sparse problems}:

% {\color{red} Begin with LAPACK, and give individual data point about how faster we are comparing to Blendenpik, set expectations about the smallest dimensions and over-determinancy ratio. }
% \begin{enumerate}\setlength{\itemsep}{-2pt}
% 	\item $A$ is dense, large, sufficiently over-determined and (approximately) rank-deficient or with unknown rank. E.g. one of the matrices we ran numerical test on has dimension $50000 \times 4000$.

% 	\item $A$ is dense, large, sufficiently over-determined with high coherence\footnote{The coherence of a matrix A is defined as the largest Euclidean norm of $U$, where $U$ is defined in the reduced singular value decomposition of $A$. }.

% 	\item $A$ is sparse, large, sufficiently over-determined with non-zero entries randomly distributed. 

% 	\item $A$ is moderately sparse (e.g. one percent of entries are non-zero), large and moderately over-determined. 

% \end{enumerate}

% In the following situations using Ski-LLS is competitive with state-of-the-arts

% \begin{enumerate}\setlength{\itemsep}{-2pt}
% 	\item $A$ is dense, large, sufficiently over-determined with full rank. 

% 	\item $A$ is sparse, large, moderately over-determined. 
% \end{enumerate}

% Table \ref{tab::rank_def_accuracy} shows that Ski-LLS is more robust than Blendenpik when the matrix $A$ is approximately rank-deficient. Figure \ref{fig::compare_blen_coherent} shows that Ski-LLS outperforms Blendenpik when the matrix $A$ is (full rank) with high coherence. Figrue \ref{fig::compare_blen_semi-coherent} and Figure \ref{fig::compare_blen_incoherent} shows that Ski-LLS is competitive with Blendenpik when the matrix $A$ is (full rank). 

% Figure \ref{fig::sparse_rand} shows that Ski-LLS runs more than 10 times faster than current state-of-the-art sparse solvers on random sparse matrices. Figure \ref{fig::density001} shows that Ski-LLS significantly outperforms its competitors when $A$ is moderately sparse, large and moderately over-determined.  Figure \ref{fig::all_solver_30}, Figure \ref{fig::all_solver_10_LSQR_5} and Figure \ref{fig::all_solver_10} showcase the competitiveness of Ski-LLS on sparse, large and moderately over determined problems. 


\section{Installing Ski-LLS}
\subsection{Dependency}
\subsection{Installation instruction}
	\subsubsection{Optional tuning of FFTW}
		(DRAFT ONLY)\\
		If one does not wish to use FFTW wisdom:
		\begin{enumerate}
			\item Put the wisdom argument=0 whenever present, in the solver routine. This does not affect compilation. 
		\end{enumerate}
If one wishes to use FFTW wisdom:
\begin{enumerate}
		\item Config.hpp has two macros, FFTW\_TIMES, FFTW\_QUANT, that will be used for \\ test\/build\_fftw\_wisdom.cpp. The build\_fftw\_wisdom will try executing fftw with input sizes = linspace( FFTW\_QUANT, FFTW\_QUANT*FFTW\_TIMES, FFTW\_QUANT), where linspace(start, finish, step) is a linear space.
	\item The user then needs to compile build_fftw_wisdom using the Makefile in test/. 
	\item After compilation, build_fftw_wisdom.out accepts two arguments: ( 0 | 1 | 2 | 3 , string), where the first argument is calibration level with 0 being the lowest and 3 being the highest, the second argument is the directory path to the generated wisdom file. 
	\item Then, the user needs to go back to Config.hpp, put the path of the generated wisdom file into the macro FFTW_WISDOM_FILE, and define FFTW_WISDOM_FLAG by \\
	 | f==0	= FFTW_ESTIMATE \\
	 | f==1	= FFTW_MEASURE \\
	 | f==2	= FFTW_PATIENT \\
	 | f==3 	= FFTW_EXHAUSTIVE \\
		where f = first argument given to build_fftw_wisdom.out in Step 3. 
	
	\item Set the wisdom argument=1 whenever present. This does not affect compilation.
	\end{enumerate}

\subsection{Test the installation}

\section{Using Ski-LLS}

This is a library of three routines for solving problem \eqref{eq::problem}. 

\begin{itemize}
	\item If the matrix $A$ is dense, {\it ls_dense_hashing_blendenpik} uses LSQR with a preconditioner built from a sketch of the matrix $A$ using Hashed-Randomised-Hadamard-Transform (HRHT) and Randomised-Column-Pivoted-QR (RCPQR) to solve \eqref{eq::problem}.

	\item If $A$ is dense and has full numerical rank, {\it ls_dense_hashing_blendenpik_noCPQR} uses LSQR with a preconditioner built from a sketch of the matrix $A$ using HRHT and Column-Pivoted-QR (CPQR) to solve \eqref{eq::problem}.

	\item   If $A$ is sparse, {\it ls_sparse_spqr} uses LSQR with a preconditioner built from a sketch of the matrix $A$ using $s-$hashing and sparse QR (SPQR) to solve \eqref{eq::problem}.
\end{itemize}

\subsection{Data structure}

\paragraph{Dense Vector and Matrix}
Ski-LLS uses C++ class Vec for dense vectors and C++ class Mat for dense matrices. 
To create a $n\times 1$ vector from already existed data, take a pointer to the data and call Vec(n, *data). 
To create a $n \times d$ matrix from already existed data, take a pointer to the data and call Mat(n, d, *data). Basic matrix operations such as return a specific row, column, or matrix-matrix and matrix vector multiplications are implemented.  
These classes are defined in include/Vec.hpp and include/impl/Mat.hpp respectively. 


\paragraph{Sparse Matrix}
Ski-LLS uses Compressed Column Data C structure for sparse matrices and vectors. The sparse data structure is from SuiteSparse \cite{10.1145/2049662.2049670}. The cs_dl and cholmod_sparse structures are defined in cs.h and cholmod_core.h respectively the in SuiteSparse/include folder. 



% In particular, the cs_dl structure used as the input matrix format for sparse linear least squares is taken from the CXSparse package from SuiteSparse. It is a C structure with 7 fields: 

% \begin{itemize}
% 	\setlength\itemsep{-0.5em}
% 	\item nzmax: maximum number of non-zeros. 

% 	\item m: number of rows. 

% 	\item n: number of columns. 

% 	\item *p: column indices (size nzmax)

% 	\item *i: row indices, size nzmax. 

% 	\item *x: numerical values of non-zeros, size nzmax. 

% 	\item nz: -1 for compressed column format. 
% \end{itemize}

% Sometimes we will also use the data structure cholmod_sparse for sparse matrix in compressed column format. It has a similar structures. For more information, see the header file or documentation of SuiteSparse.

% \paragraph{Examples}

% We give two examples of constructing dense and sparse matrices from user given data. 

% The below code snippet creates a dense matrix and a vector.

% \begin{lstlisting}
% 	long m = 4;
% 	long n = 2;
% 	double data_A[8] = {0.306051,-1.53078,1.64493,-1.61322,
% 		-0.2829,0.474476,-0.586278,-0.610202};

% 	double data_b[4] = {0.0649338,0.845946,-0.0164085,0.247119};

% 	Mat_d A(m,n,data_A);
% 	Vec_d b(m,data_b);
% \end{lstlisting}

% The below code snippet creates a sparse matrix.

% \begin{lstlisting}
%     long m = 4;
%     long n = 3;
%     long nnz = 5;

%     long col[4] = {1,  2,  4,  6};
%     long row[5] = {1,  1,  3,  2,  4}; 
%     double val[5] = {2.0,  3.0,  1.0,  4.0,  5.0};
%     long col_0[4];
%     long row_0[5];

%     // convert to zero based indices
%     for (long i=0; i<n+1; i++){
%      col_0[i] = col[i]-1;
%     }

%     for (long i =0; i<nnz; i++){
%      row_0[i] = row[i]-1;
%     }

%     cholmod_common Common, *cc;
%     cc = &Common;
%     cholmod_l_start(cc);
%     cc->print = 4;

%     cholmod_sparse* A;
%     A = cholmod_l_allocate_sparse(
%     m, n, nnz, true, true, 0, CHOLMOD_REAL, cc);

%     A->p = col_0;
%     A->i = row_0;
%     A->x = val;

%     cholmod_l_print_sparse( A, "A", cc);

%     cholmod_l_finish(cc);
% \end{lstlisting}





\subsection{Dense problems}
% // C++ solver for dense linear least squares using hashing
% // Solving the linear least sqaures min_x |Ax-b|_2

To compute a solution of a linear least square problem where the matrix $A$ is dense, a call of the following form should be made. \\

ls_dense_hashing_blendenpik(A, b, x, rank, flag, it, gamma, k, abs_tol, rcond, it_tol, max_it, debug, wisdom):
	\begin{itemize}
	\setlength\itemsep{-0.5em}
	\item Inputs 
		\begin{enumerate}
			\item $\mathbf A$ is type Mat_d containing a $\R^{m\times n}$ matrix defined in \eqref{eq::problem}.
			\item $\mathbf b$ is type Vec_d containing a $\R^m$ vector defined in \eqref{eq::problem}.
		\end{enumerate}
	
	\item Main Outputs
		\begin{enumerate}
			\item { $\mathbf x$} is type Vec_d containing a $\R^n$ vector which is a solution of \eqref{eq::problem}.
		\end{enumerate}

	\item Auxillary Outputs
		\begin{enumerate}
			\item {\bf rank} is a scalar containing the detected numerical rank of the matrix $A$. 
			\item {\bf flag} is a scaler. flag=0 indicates LSQR has converged. flag=1 indicates LSQR has not converged. 
			\item {\bf it} is a scaler indicating the number of LSQR iterations taken. 
		\end{enumerate}
		

	\item Parameters
		\begin{enumerate}
			\item {\bf gamma} is the over-sampling ratio used in sketching. Bigger gamma typically gives higher quality preconditioner but slower running time. The default value of gamma is $1.7$.

			\item $\mathbf k$ is number of non-zeros per column in the hashing matrix as part of sketching. Bigger $k$ typically gives higher quality preconditioner but slower running time. The default value of $k$ is 1. 

			\item {\bf abs_tol} specifies an absolute residual tolerance for the solution $x$ of \ref{eq::problem}. If the algorithm finds an $x$ satisfies $\|Ax-b\|_2 \leq abs_tol$, it terminates and returns $x$. The default value is $10^{-8}$

			\item {\bf it_tol} specifies the relative residual tolerance for LSQR convergence, see \cite{Paige:1982aa}. The default value is $10^{-6}$.

			 \item {\bf max_it} specifies the maximum iteration of LSQR, the default value is $10,000$. 


			 \item {\bf debug} is a flag. If debug=1, the solver prints additional outputs for diagosis. 

			 \item {\bf wisdom} is a flag. If wisdom=1, the macro variable FFTW_WISDOM_FILE in inclde/Config.hpp must be defined as the path to a FFTW wisdom file, see fftw documentation at fftw.org. If wisdom=0, the solver does not use FFTW wisdom and may run slower. 
		\end{enumerate}
		

	\end{itemize}


If the matrix $A$ is known to be of full numerical rank, then a different routine can be used which is slightly faster:

ls_dense_hashing_blendenpik_noCPQR(A, b, x, rank, flag, it, gamma, k, it_tol, max_it, debug, wisdom). The arguments usage is the same as above, but we don't need the rank-detection parameter rcond, and the shortcut related to abs_tol is not implemented.

\paragraph{Example}

The following program (test/ski-llsDenseExample.cpp) solves an example of problem \eqref{eq::problem} with given $A$ and $b$, where 
\begin{equation}
A = \begin{pmatrix}
0.306051 & -1.53078 \\
1.64493 & -1.61322 \\
-0.2829 & 0.474476	\\
-0.586278 & -0.610202 
\end{pmatrix} \quad \text{and} \quad 
b = \begin{pmatrix}
0.0649338 \\
0.845946 \\
-0.0164085\\
0.247119 & 
\end{pmatrix}.
\end{equation}

\begin{lstlisting}
#include <iostream>
#include "ski-lls.h" 

int main(int argc, char **argv)
{
	// create data
	long m = 4;
	long n = 2;
	double data_A[8] = {0.306051,-1.53078,1.64493,-1.61322,
	-0.2829,0.474476,-0.586278,-0.610202}; 

	double data_b[4] = {0.0649338,0.845946,-0.0164085,0.247119};

	Mat_d A(m,n,data_A);
	Vec_d b(m,data_b);

	// parameters
	long k = NNZ_PER_COLUMN;
	double gamma = OVER_SAMPLING_RATIO;
	long max_it = MAX_IT;
	double it_tol = IT_TOL;
	double rcond = 1e-10;
	int wisdom = 0;
	int debug = 0;
	double abs_tol = ABS_TOL;

	// storage

	long it;
	int flag;
	long rank;
	double residual;   
	Vec_d x(A.n());

	// solve
	ls_dense_hashing_blendenpik(
	A, b, x, rank, flag, it, gamma, k, abs_tol, 
	rcond, it_tol, max_it, debug, wisdom);

	std::cout << "A is: "<< A << std::endl;
	std::cout << "b is: "<< b << std::endl;
	std::cout << "solution is: "<< x << std::endl;

	// compute residual
	A.mv('n', 1, x, -1, b); 
	std::cout << "Residual of ls_blendenpik_hashing is: "<<  nrm2(b) << std::endl;
	std::cout << "Iteration of ls_blendenpik_hashing is: " << it << std::endl;
	// The correct value is x = (-0.2122, 0.720)

}
\end{lstlisting}


\subsection{Sparse problems}

To compute a solution of a linear least square problem where the matrix $A$ is sparse, a call of the following form should be made. \\

ls_sparse_spqr(*A, b, x, 
    rank, flag, it, gamma, k, abs_tol, ordering, it_tol, max_it, rcond, peturb, debug):

	\begin{itemize}
	\setlength\itemsep{-0.5em}
	\item Inputs 
		\begin{enumerate}
			\item $\mathbf A$ is pointer to type cs_dl containing a $\R^{m\times n}$ sparse matrix defined in \eqref{eq::problem}.
			\item $\mathbf b$ is type Vec_d containing a $\R^m$ vector defined in \eqref{eq::problem}.
		\end{enumerate}
	
	\item Main Outputs
		\begin{enumerate}
			\item {$\mathbf x$} is type Vec_d containing a $\R^n$ vector which is a solution of \eqref{eq::problem}.
		\end{enumerate}

	\item Auxillary Outputs
		\begin{enumerate}
			\item {\bf rank} is a scalar containing the detected numerical rank of the matrix $A$. 
			\item {\bf flag} is a scaler. flag=0 indicates LSQR has converged. flag=1 indicates LSQR has not converged. 
			\item {\bf it} is a scaler indicating the number of LSQR iterations taken. 
		\end{enumerate}
		

	\item Parameters
		\begin{enumerate}
			\item {\bf gamma} is the over-sampling ratio used in sketching. Bigger gamma typically gives higher quality preconditioner but slower running time. The default value of gamma is $1.7$.

			\item $\mathbf k$ is number of non-zeros per column in the hashing matrix as part of sketching. Bigger $k$ typically gives higher quality preconditioner but slower running time. The default value of $k$ is 1. 

			\item {\bf abs_tol} specifies an absolute residual tolerance for the solution $x$ of \ref{eq::problem}. If the algorithm finds an $x$ satisfies $\|Ax-b\|_2 \leq abs_tol$, it terminates and returns $x$. The default value is $10^{-8}$

			\item {\bf ordering} is an integer specifying a fill-reduced ordering for sparse QR factorization. The default value is {SPQR_ORDERING_DEFAULT}. See SuiteSparseQR documentation for more information. 

			\item {\bf it_tol} specifies the relative residual tolerance for LSQR convergence, see \cite{Paige:1982aa}. The default value is $10^{-6}$.

			\item {\bf max_it} specifies the maximum iteration of LSQR, the default value is $10,000$. 

			\item {\bf rcond} is numerical-ill-conditioning tolerance for the preconditioner returned by sparse QR. If the condition number of the preconditioner is larger than $1/rcond$, a warning will be printed. The default value of rcond is $10^{-12}$

			\item {\bf perturb} is a real number. It has no-effect on the algorithm and is part of an ongoing development. 

			\item {\bf debug} is a flag. If debug=1, the solver prints additional outputs for diagosis. 
		\end{enumerate}
		

	\end{itemize}

\paragraph{Example}

The following program (test/ski-llsSparseExample.cpp) solves an example of problem \eqref{eq::problem} with given sparse $A$ and dense $b$, where 
\begin{equation}
A = \begin{pmatrix}
     2     & 3     & 0\\
     0     & 0     & 4\\
     0     & 1     & 0\\
     0     & 0     & 5
\end{pmatrix} \quad \text{and} \quad 
b = \begin{pmatrix}
0.0649338 \\
0.845946 \\
-0.0164085\\
0.247119 & 
\end{pmatrix}.
\end{equation}

\begin{lstlisting}
#include <iostream>
#include "ski-lls.h"

int main(int argc, char **argv)
{
	// Create a sparse matrix in compressed column format
    long m = 4;
    long n = 3;
    long nnz = 5;

    long col[4] = {0,  1,  3,  5};
    long row[5] = {0,  0,  2,  1,  3}; 
    double val[5] = {2.0,  3.0,  1.0,  4.0,  5.0};

    cs_dl* A;
    A = cs_dl_spalloc(m, n, nnz, 1,0 );

    A->p = (long*)col;
    A->i = (long*)row;
    A->x = (double*)val;


    double data_b[4] = {0.0649338,0.845946,-0.0164085,0.247119};
    Vec_d b(m, data_b);

    // parameters
    long k = NNZ_PER_COLUMN;
    double gamma = OVER_SAMPLING_RATIO;
    long max_it = MAX_IT;
    double it_tol = IT_TOL;
    double abs_tol = ABS_TOL;
    double rcond = 1e-12;
    int ordering = SPQR_ORDERING;
    int debug = 0;

    // storage

    long it;
    int flag;
    long rank;
    double t_start;
    double t_finish;
    double residual;   
    Vec_d x(A->n);

    // solve
    ls_sparse_spqr(*A, b, x, 
    rank, flag, it, gamma, k, abs_tol, ordering, it_tol, max_it, RCOND_THRESHOLD, PERTURB, debug);

    // compute residual
    CSC_Mat_d A_CSC(A->m, A->n, A->nzmax, 
        (long*)A->i, (long*)A->p, (double*)A->x);
    A_CSC.mv('n', 1, x, -1, b);

    std::cout << "A is: " << std::endl;
    cs_dl_print(A,0);
    std::cout << "b is: "<< b << std::endl;
    std::cout << "solution is: "<< x << std::endl;
    // Should get     x = (0.0571 -0.0164 0.1127)

    std::cout << "Residual of ls_sparse_spqr is: "<<  nrm2(b) << std::endl;

}
\end{lstlisting}

\bibliography{bib/lib_20Aug.bib} 
\bibliographystyle{abbrv}

\appendix

\section{Numerical Results}
\begin{table}[H]
\scriptsize
\centering
\begin{tabular}{l|rrrrrrr}
               & lp\_ship12l                          & Franz1               & GL7d26                                & cis-n4c6-b2          & lp\_modszk1                           & rel5                                  & ch5-5-b1              \\ 
\hline
SVD          & 18.336                               & 26.503               & 50.875                                & 6.1E-14              & 33.236                                & 14.020                                & 7.3194                \\
Blendenpk      & \multicolumn{1}{l}{~~~~~~~~~NaN~~~~} & 9730.700             & \multicolumn{1}{l}{~~~~~~~~~~NaN~~~~} & 3.0E+02              & \multicolumn{1}{l}{~~~~~~~~~~NaN~~~~} & \multicolumn{1}{l}{~~~~~~~~~~NaN~~~~} & 340.9200              \\
Ski-LLS-dense  & 18.336                               & 26.503               & 50.875                                & 5.3E-14              & 33.236                                & 14.020                                & 7.3194                \\
               & \multicolumn{1}{l}{}                 & \multicolumn{1}{l}{} & \multicolumn{1}{l}{}                  & \multicolumn{1}{l}{} & \multicolumn{1}{l}{}                  & \multicolumn{1}{l}{}                  & \multicolumn{1}{l}{}  \\
               & n3c5-b2                              & ch4-4-b1             & n3c5-b1                               & n3c4-b1              & connectus                             & landmark                              & cis-n4c6-b3           \\ 
\hline
SVD            & 9.0E-15                              & 4.2328               & 3.4641                                & 1.8257               & 282.67                                & 1.1E-05                               & 30.996                \\
Blendenpk      & 1.3E+02                              & 66.9330              & 409.8000                              & 8.9443               & \multicolumn{1}{l}{~~~~~~~~~~NaN~~~~} & \multicolumn{1}{l}{~~~~~~~~~~NaN~~~~} & 3756.200              \\
Ski-LLS-dense  & 5.2E-15                              & 4.2328               & 3.4641                                & 1.8257               & 282.67                                & 1.1E-05                               & 30.996               
\end{tabular}
\caption{Residual of solvers for a range of approximate rank-deficient problems taken from the Florida matrix collection \cite{10.1145/2049662.2049663}. We see while Blendenpik struggles, Ski-LLS achives the same residual accuracy as SVD method.}
\label{tab::rank_def_accuracy}
\end{table}





\end{document}